{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Needed Packages\n",
    "If any of the packages can't be imported, installation must be done first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "from scipy.io import arff\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of class to be predicted: 4\n",
    "- Fixation\n",
    "- Smooth Pursuit\n",
    "- Saccade\n",
    "- Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['time',\n",
       " 'x',\n",
       " 'y',\n",
       " 'confidence',\n",
       " 'handlabeller1',\n",
       " 'handlabeller2',\n",
       " 'handlabeller_final']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Loading the arff file\n",
    "file_dir = 'Dataset/GazeCom/GazeCom_ground_truth/beach/AAF_beach.arff' ## Replace with the directory of the selected file in the dataset\n",
    "arff_overview = arff.loadarff(file_dir)\n",
    "\n",
    "## Checking the attributes of the file\n",
    "arff_overview[1].names()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset description:\n",
    "- label to use: handlabeller_final\n",
    "- values:\n",
    "- 1.0 = Fixation\n",
    "- 2.0 = Saccades\n",
    "- 3.0 = Smooth Pursuit\n",
    "- 4.0 = Noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>confidence</th>\n",
       "      <th>handlabeller1</th>\n",
       "      <th>handlabeller2</th>\n",
       "      <th>handlabeller_final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>590.9</td>\n",
       "      <td>5.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>590.9</td>\n",
       "      <td>5.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9000.0</td>\n",
       "      <td>590.6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13000.0</td>\n",
       "      <td>590.4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17000.0</td>\n",
       "      <td>589.8</td>\n",
       "      <td>5.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5003</th>\n",
       "      <td>20124000.0</td>\n",
       "      <td>707.6</td>\n",
       "      <td>665.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5004</th>\n",
       "      <td>20128000.0</td>\n",
       "      <td>706.0</td>\n",
       "      <td>668.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5005</th>\n",
       "      <td>20132000.0</td>\n",
       "      <td>706.7</td>\n",
       "      <td>670.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5006</th>\n",
       "      <td>20136000.0</td>\n",
       "      <td>707.6</td>\n",
       "      <td>675.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5007</th>\n",
       "      <td>20140000.0</td>\n",
       "      <td>708.0</td>\n",
       "      <td>678.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5008 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            time      x      y  confidence  handlabeller1  handlabeller2  \\\n",
       "0         1000.0  590.9    5.2         1.0            4.0            4.0   \n",
       "1         5000.0  590.9    5.2         1.0            4.0            4.0   \n",
       "2         9000.0  590.6    5.0         1.0            4.0            4.0   \n",
       "3        13000.0  590.4    5.0         1.0            4.0            4.0   \n",
       "4        17000.0  589.8    5.2         1.0            4.0            4.0   \n",
       "...          ...    ...    ...         ...            ...            ...   \n",
       "5003  20124000.0  707.6  665.2         1.0            1.0            1.0   \n",
       "5004  20128000.0  706.0  668.1         1.0            1.0            1.0   \n",
       "5005  20132000.0  706.7  670.2         1.0            1.0            1.0   \n",
       "5006  20136000.0  707.6  675.2         1.0            1.0            1.0   \n",
       "5007  20140000.0  708.0  678.6         1.0            1.0            1.0   \n",
       "\n",
       "      handlabeller_final  \n",
       "0                    4.0  \n",
       "1                    4.0  \n",
       "2                    4.0  \n",
       "3                    4.0  \n",
       "4                    4.0  \n",
       "...                  ...  \n",
       "5003                 1.0  \n",
       "5004                 1.0  \n",
       "5005                 1.0  \n",
       "5006                 1.0  \n",
       "5007                 1.0  \n",
       "\n",
       "[5008 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Reading the arff file using Pandas DataFrame\n",
    "df_overview = pd.DataFrame(arff_overview[0])\n",
    "df_overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is meant to:\n",
    "- Get new features from the dataset: speed, acceleration, diretion, standard deviation, and distance from several temporal window sizes\n",
    "- Converting the arff files, add the new features, and saving them in .csv format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_end(i, step, window, conf_min, df):\n",
    "    if step == window: ## applies for window size of 1\n",
    "        start = i - step\n",
    "        end = i\n",
    "    else:\n",
    "        start = i - step\n",
    "        end = i + step\n",
    "    \n",
    "    if start < 0 or df['confidence'][start] < conf_min:\n",
    "        start = i\n",
    "    if end >= len(df) or df['confidence'][end] < conf_min:\n",
    "        end = i \n",
    "    \n",
    "    return start, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_velocity(df, speed_col_name, direction_col_name, window):\n",
    "    ## set 0 values, must be float!\n",
    "    df[speed_col_name] = 0.0\n",
    "    df[direction_col_name] = 0.0\n",
    "\n",
    "    step = np.math.ceil(window/2)\n",
    "    conf_min = 0.75\n",
    "    for i in range(0, len(df)):\n",
    "        if df['confidence'][i] < conf_min:\n",
    "            continue\n",
    "        start, end = get_start_end(i, step, window, conf_min, df)\n",
    "        if start == end:\n",
    "            continue\n",
    "        difference_x = df['x'][end] - df['x'][start]\n",
    "        difference_y = df['y'][end] - df['y'][start]\n",
    "        \n",
    "        hypotenuse = np.math.sqrt(difference_x**2 + difference_y**2)\n",
    "        time_delta = (df['time'][end] - df['time'][start])/1000000 ## time in microseconds, convert to seconds\n",
    "\n",
    "        ## Assigning\n",
    "        df[speed_col_name][i] = hypotenuse/time_delta\n",
    "        df[direction_col_name][i] = np.math.atan2(difference_y, difference_x) ## calculate the arctangent of delta_y/delta_x\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_std(df, std_col_name, displacement_col_name, window):\n",
    "    ## set 0 values, must be float!\n",
    "    df[std_col_name] = 0.0\n",
    "    df[displacement_col_name] = 0.0\n",
    "\n",
    "    step = np.math.ceil(window/2)\n",
    "    conf_min = 0.75\n",
    "    for i in range(0, len(df)):\n",
    "        if df['confidence'][i] < conf_min:\n",
    "            continue\n",
    "        start, end = get_start_end(i, step, window, conf_min, df)\n",
    "        if start == end:\n",
    "            continue\n",
    "        list_x, list_y = [], []\n",
    "        displacement_x, displacement_y = 0, 0\n",
    "        for j in range(start, end + 1):\n",
    "            list_x.append(df['x'][j])\n",
    "            list_y.append(df['y'][j])\n",
    "        ## Calculating the difference between all set of courses\n",
    "        for j in range(start, end):\n",
    "            displacement_x += df['x'][j + 1] - df['x'][j]\n",
    "            displacement_y += df['y'][j + 1] - df['y'][j]\n",
    "        \n",
    "        hypotenuse = np.math.sqrt(displacement_x**2 + displacement_y**2)\n",
    "        ## Calculating the standard deviation of x's and y's over the full window, calculating the mean between x and y\n",
    "        std_x = np.std(list_x)\n",
    "        std_y = np.std(list_y)\n",
    "        std = np.mean([std_x, std_y])\n",
    "        \n",
    "        ## Assigning\n",
    "        df[std_col_name][i] = std\n",
    "        df[displacement_col_name][i] = hypotenuse\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acceleration(df, accl_col_name, speed_col_name, direction_col_name, window):\n",
    "    ## set 0 values, must be float!\n",
    "    df[accl_col_name] = 0.0\n",
    "    w = window\n",
    "    window = 1\n",
    "\n",
    "    conf_min = 0.75\n",
    "    step = np.math.ceil(window/2)\n",
    "    for i in range(0, len(df)):\n",
    "        if df['confidence'][i] < conf_min:\n",
    "            continue\n",
    "        start, end = get_start_end(i, step, window, conf_min, df)\n",
    "        if start == end:\n",
    "            continue\n",
    "        ## Vx = Vo*cos(alpha)\n",
    "        v_start_x = df[speed_col_name][start]*np.math.cos(df[direction_col_name][start])\n",
    "        v_end_x = df[speed_col_name][end]*np.math.cos(df[direction_col_name][end])\n",
    "        \n",
    "        ## Vy = Vo*sin(alpha)\n",
    "        v_start_y = df[speed_col_name][start]*np.math.sin(df[direction_col_name][start])\n",
    "        v_end_y = df[speed_col_name][end]*np.math.sin(df[direction_col_name][end])\n",
    "\n",
    "        time_delta = (df['time'][end] - df['time'][start])/1000000 ## time in microseconds, convert to seconds\n",
    "        \n",
    "        ## Calculating each dimension's acceleration\n",
    "        accl_x = (v_end_x - v_start_x)/time_delta\n",
    "        accl_y = (v_end_y - v_start_y)/time_delta\n",
    "        accl = np.math.sqrt(accl_x**2 + accl_y**2)\n",
    "        \n",
    "        ## Assigning\n",
    "        df[accl_col_name][i] = accl\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df, window_size):\n",
    "    temp = df.copy()\n",
    "    for i in window_size:\n",
    "        speed_col_name = 'speed_' + str(i)\n",
    "        direction_col_name = 'direction_' + str(i)\n",
    "        std_col_name = 'std_' + str(i)\n",
    "        displacement_col_name = 'displacement_' + str(i)\n",
    "        accl_col_name = 'acceleration_' + str(i)\n",
    "\n",
    "        temp = get_velocity(temp, speed_col_name, direction_col_name, i)\n",
    "        temp = get_acceleration(temp, accl_col_name, speed_col_name, direction_col_name, i)\n",
    "        temp = get_std(temp, std_col_name, displacement_col_name, i)   \n",
    "\n",
    "    df = temp\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beach',\n",
       " 'breite_strasse',\n",
       " 'bridge_1',\n",
       " 'bridge_2',\n",
       " 'bumblebee',\n",
       " 'doves',\n",
       " 'ducks_boat',\n",
       " 'ducks_children',\n",
       " 'golf',\n",
       " 'holsten_gate',\n",
       " 'koenigstrasse',\n",
       " 'puppies',\n",
       " 'roundabout',\n",
       " 'sea',\n",
       " 'st_petri_gate',\n",
       " 'st_petri_market',\n",
       " 'st_petri_mcdonalds',\n",
       " 'street']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get a list of all the video names in the dataset. The path can be changed depending on where the file is located.\n",
    "video_param = json.load(open('Dataset/GazeCom/GazeCom_video_parameters.json'))\n",
    "video_names = video_param['video_names']\n",
    "video_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set window sizes\n",
    "window_size = [1, 2, 4, 8, 16, 32, 64, 128] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing folder: st_petri_mcdonalds\n",
      "\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/st_petri_mcdonalds\\AAF_st_petri_mcdonalds.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/st_petri_mcdonalds\\ALK_st_petri_mcdonalds.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/st_petri_mcdonalds\\APS_st_petri_mcdonalds.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/st_petri_mcdonalds\\C1K_st_petri_mcdonalds.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/st_petri_mcdonalds\\CCB_st_petri_mcdonalds.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/st_petri_mcdonalds\\CCE_st_petri_mcdonalds.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/st_petri_mcdonalds\\CCF_st_petri_mcdonalds.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/st_petri_mcdonalds\\CCK_st_petri_mcdonalds.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/st_petri_mcdonalds\\CCM_st_petri_mcdonalds.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/st_petri_mcdonalds\\DDT_st_petri_mcdonalds.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/st_petri_mcdonalds\\FFS_st_petri_mcdonalds.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/st_petri_mcdonalds\\FTD_st_petri_mcdonalds.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/st_petri_mcdonalds\\GGM_st_petri_mcdonalds.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/st_petri_mcdonalds\\HHB_st_petri_mcdonalds.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/st_petri_mcdonalds\\I1H_st_petri_mcdonalds.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/st_petri_mcdonalds\\INH_st_petri_mcdonalds.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/st_petri_mcdonalds\\JJB_st_petri_mcdonalds.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/st_petri_mcdonalds\\JJL_st_petri_mcdonalds.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/st_petri_mcdonalds\\JJR_st_petri_mcdonalds.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/st_petri_mcdonalds\\JMT_st_petri_mcdonalds.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/st_petri_mcdonalds\\JSH_st_petri_mcdonalds.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/st_petri_mcdonalds\\KKD_st_petri_mcdonalds.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/st_petri_mcdonalds\\KKH_st_petri_mcdonalds.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/st_petri_mcdonalds\\KKW_st_petri_mcdonalds.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/st_petri_mcdonalds\\MBS_st_petri_mcdonalds.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/st_petri_mcdonalds\\MKB_st_petri_mcdonalds.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/st_petri_mcdonalds\\MMD_st_petri_mcdonalds.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/st_petri_mcdonalds\\MMS_st_petri_mcdonalds.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/st_petri_mcdonalds\\MNH_st_petri_mcdonalds.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/st_petri_mcdonalds\\NEZ_st_petri_mcdonalds.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/st_petri_mcdonalds\\NNS_st_petri_mcdonalds.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/st_petri_mcdonalds\\RRP_st_petri_mcdonalds.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/st_petri_mcdonalds\\RSS_st_petri_mcdonalds.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/st_petri_mcdonalds\\S1S_st_petri_mcdonalds.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/st_petri_mcdonalds\\SIS_st_petri_mcdonalds.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/st_petri_mcdonalds\\SSC_st_petri_mcdonalds.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/st_petri_mcdonalds\\SSH_st_petri_mcdonalds.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/st_petri_mcdonalds\\SSK_st_petri_mcdonalds.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/st_petri_mcdonalds\\SSS_st_petri_mcdonalds.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/st_petri_mcdonalds\\TTS_st_petri_mcdonalds.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/st_petri_mcdonalds\\VVB_st_petri_mcdonalds.arff\n",
      "Preprocessing folder: street\n",
      "\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/street\\AAF_street.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/street\\AAW_street.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/street\\ALK_street.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/street\\APS_street.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/street\\C1K_street.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/street\\CCB_street.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/street\\CCE_street.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/street\\CCF_street.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/street\\CCK_street.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/street\\CCM_street.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/street\\DDT_street.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/street\\FFS_street.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/street\\FTD_street.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/street\\GGM_street.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/street\\HHB_street.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/street\\I1H_street.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/street\\INH_street.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/street\\JJB_street.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/street\\JJK_street.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/street\\JJL_street.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/street\\JJR_street.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/street\\JMT_street.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/street\\JSH_street.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/street\\K1B_street.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/street\\KKB_street.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/street\\KKD_street.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/street\\KKS_street.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/street\\KKW_street.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/street\\M1K_street.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/street\\MBS_street.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/street\\MKB_street.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/street\\MMD_street.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/street\\MMS_street.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/street\\MNH_street.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/street\\NEZ_street.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/street\\NNS_street.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/street\\RRP_street.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/street\\RSS_street.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/street\\S1S_street.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/street\\SAB_street.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/street\\SIS_street.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/street\\SJS_street.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/street\\SSC_street.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/street\\SSH_street.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/street\\SSK_street.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/street\\SSS_street.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/street\\TTS_street.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/street\\VVB_street.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/street\\VVK_street.arff\n",
      "Preprocessing file: Dataset/GazeCom/GazeCom_ground_truth/street\\YFK_street.arff\n"
     ]
    }
   ],
   "source": [
    "## Adding new features to all .arff files in the ground_truth folder.\n",
    "for name in video_names:\n",
    "    print(\"Preprocessing folder: \" + str(name))\n",
    "    print('')\n",
    "    for filename in glob.glob(r'Dataset/GazeCom/GazeCom_ground_truth/' + str(name) + '/*.arff'):\n",
    "        arff_file = arff.loadarff(filename)\n",
    "        print(\"Preprocessing file: \" + str(filename))\n",
    "        df = pd.DataFrame(arff_file[0])\n",
    "\n",
    "        df = preprocessing(df, window_size)\n",
    "\n",
    "        ## Save to new folder\n",
    "        df.to_csv(f'Dataset/GazeCom/GazeCom_preprocessed_final/{name}/{os.path.basename(filename)[:-5]}.csv', index = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is meant to do:\n",
    "- All the .csv files in the new folder will be augmented into a single HDF5 file.\n",
    "- Before the augmentation, a feature selection step is employed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`keys_to_keep`: All the features that will be divided by the PPD constant\n",
    "\n",
    "`keys_all`: All the features that will be augmented inside the HDF5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_keep = ['speed', 'direction', 'acceleration']\n",
    "windows_to_keep = 7\n",
    "keys_to_keep = []\n",
    "\n",
    "if 'xy' in features_to_keep:\n",
    "    keys_to_keep += ['x', 'y']\n",
    "\n",
    "if 'speed' in features_to_keep:\n",
    "    keys_to_keep += ['speed_{}'.format(i) for i in (1, 2, 4, 8, 16, 32, 64, 128)[:windows_to_keep]]\n",
    "if 'direction' in features_to_keep:\n",
    "    keys_to_keep += ['direction_{}'.format(i) for i in (1, 2, 4, 8, 16, 32, 64, 128)[:windows_to_keep]]\n",
    "if 'acceleration' in features_to_keep:\n",
    "    keys_to_keep += ['acceleration_{}'.format(i) for i in (1, 2, 4, 8, 16, 32, 64, 128)[:windows_to_keep]]\n",
    "if 'stddev' in features_to_keep:\n",
    "    keys_to_keep += ['std_{}'.format(i) for i in (1, 2, 4, 8, 16, 32, 64, 128)[:windows_to_keep]]\n",
    "if 'displacement' in features_to_keep:\n",
    "    keys_to_keep += ['displacement_{}'.format(i) for i in (1, 2, 4, 8, 16, 32, 64, 128)[:windows_to_keep]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_all = []\n",
    "windows_all = 8\n",
    "\n",
    "keys_all += ['x', 'y']\n",
    "keys_all += ['speed_{}'.format(i) for i in (1, 2, 4, 8, 16, 32, 64, 128)[:windows_all]]\n",
    "keys_all += ['direction_{}'.format(i) for i in (1, 2, 4, 8, 16, 32, 64, 128)[:windows_all]]\n",
    "keys_all += ['acceleration_{}'.format(i) for i in (1, 2, 4, 8, 16, 32, 64, 128)[:windows_all]]\n",
    "keys_all += ['std_{}'.format(i) for i in (1, 2, 4, 8, 16, 32, 64, 128)[:windows_all]]\n",
    "keys_all += ['displacement_{}'.format(i) for i in (1, 2, 4, 8, 16, 32, 64, 128)[:windows_all]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_to_convert_to_degrees = ['x', 'y'] + [k for k in keys_to_keep if 'speed_' in k or 'acceleration_' in k]\n",
    "keys_to_convert_to_degrees = set(keys_to_convert_to_degrees).intersection(keys_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding attributes, this is general to the GazeCom dataset and might be different for different types of eye movement datasets\n",
    "width_px = 1280\n",
    "height_px = 720\n",
    "width_mm = 400\n",
    "height_mm = 225.0\n",
    "distance = 450.0\n",
    "\n",
    "## Creating a function to calculate the PPD\n",
    "def calculate_ppd(width_px, height_px, width_mm, height_mm, distance):\n",
    "    theta_w = 2 * math.atan(width_mm / (2 * distance)) * 180. / math.pi\n",
    "    theta_h = 2 * math.atan(height_mm / (2 * distance)) * 180. / math.pi\n",
    "\n",
    "    ppdx = width_px / theta_w\n",
    "    ppdy = height_px / theta_h\n",
    "\n",
    "    return (ppdx + ppdy) / 2\n",
    "\n",
    "ppd = calculate_ppd(width_px, height_px, width_mm, height_mm, distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.178149399649236"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Checking the value of the PPD constant\n",
    "ppd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['x', 'y', 'speed_1', 'direction_1', 'acceleration_1', 'std_1',\n",
      "       'displacement_1', 'speed_2', 'direction_2', 'acceleration_2', 'std_2',\n",
      "       'displacement_2', 'speed_4', 'direction_4', 'acceleration_4', 'std_4',\n",
      "       'displacement_4', 'speed_8', 'direction_8', 'acceleration_8', 'std_8',\n",
      "       'displacement_8', 'speed_16', 'direction_16', 'acceleration_16',\n",
      "       'std_16', 'displacement_16', 'speed_32', 'direction_32',\n",
      "       'acceleration_32', 'std_32', 'displacement_32', 'speed_64',\n",
      "       'direction_64', 'acceleration_64', 'std_64', 'displacement_64',\n",
      "       'speed_128', 'direction_128', 'acceleration_128', 'std_128',\n",
      "       'displacement_128'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "num_classes = 5\n",
    "preprocessed_location = 'Dataset/GazeCom/GazeCom_preprocessed_final/' + '/{}/*.csv'\n",
    "x = []\n",
    "y = []\n",
    "y_onehot = []\n",
    "\n",
    "## Augmenting process\n",
    "for i in video_names:\n",
    "    fnames = sorted(glob.glob(preprocessed_location.format(i)))\n",
    "\n",
    "    x.append([])\n",
    "    y.append([])\n",
    "    y_onehot.append([])\n",
    "    for f in fnames:\n",
    "        df = pd.read_csv(f, index_col = 0)\n",
    "        df_label = df['handlabeller_final']\n",
    "        df = df.drop(columns = ['time', 'confidence', 'handlabeller1', 'handlabeller2', 'handlabeller_final'])\n",
    "        for k in keys_to_convert_to_degrees:\n",
    "            df[k] /= ppd\n",
    "        x[-1].append(np.hstack([np.reshape(np.array(df[key]), (-1, 1)) for key in keys_all]))\n",
    "        assert x[-1][-1].dtype == np.float64\n",
    "        y[-1].append(df_label.astype('int64'))\n",
    "        y_onehot[-1].append(np.eye(num_classes)[y[-1][-1]])\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving the augmented data into HDF5 file\n",
    "path_h5 = 'Dataset/GazeCom/GazeCom_h5/GazeCom_preprocessed_fixed_ppd_720.h5'\n",
    "pickle.dump({'data_X': x, 'data_Y': y, 'data_Y_one_hot': y_onehot},\n",
    "                        open(path_h5, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
